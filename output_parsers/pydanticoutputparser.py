from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

load_dotenv()

llm = HuggingFaceEndpoint(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    task="text-generation"
)

model = ChatHuggingFace(llm=llm)

class Person(BaseModel):
    name: str = Field(description = "Name of the person")
    age: int = Field(gt=18, description = "Age of the person")
    city: str = Field(description = "City of the person")


# LangChain wraps your Person model
# This parser has two jobs:
# Tell the LLM how to format its output
# Validate + convert raw text → Person object
parser = PydanticOutputParser(pydantic_object=Person)


# .get_format_instructions():
# They are auto-generated by PydanticOutputParser and injected into the prompt via
# parser.get_format_instructions().

# So the LLM doesn’t guess.
# It’s explicitly told what format to follow.

# Final prompt sent to the LLM realistically is this :
# Generate the name, age and city of a fictional sri lankan person

# The output should be formatted as a JSON instance that conforms to the JSON schema below.
# {
#   "name": "string",
#   "age": "integer > 18",
#   "city": "string"
# }


template = PromptTemplate(
    template = 'Generate the name, age, city of a fictional {place} person \n {format_instructions}',
    input_variables = ['place'],
    partial_variables = {'format_instructions': parser.get_format_instructions()}
)

chain = template | model | parser

result = chain.invoke({'place': 'UAE'})

print(result)